{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['target'] = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, [0,1,2, 3]].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# oh = OneHotEncoder()\n",
    "# yTrain = oh.fit_transform(yTrain).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "probs_y=classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test     | y_pred     | Setosa(%)  | versicolor(%) | virginica(%)\n",
      "-----------------------------------------------------------------\n",
      "2          | 2          | 0.0        | 0.03          | 0.97      \n",
      "1          | 1          | 0.01       | 0.95          | 0.04      \n",
      "0          | 0          | 1.0        | 0.0           | 0.0       \n",
      "2          | 2          | 0.0        | 0.08          | 0.92      \n",
      "0          | 0          | 0.98       | 0.02          | 0.0       \n",
      "2          | 2          | 0.0        | 0.01          | 0.99      \n",
      "0          | 0          | 0.98       | 0.02          | 0.0       \n",
      "1          | 1          | 0.01       | 0.71          | 0.28      \n",
      "1          | 1          | 0.0        | 0.73          | 0.27      \n",
      "1          | 1          | 0.02       | 0.89          | 0.08      \n",
      "2          | 2          | 0.0        | 0.44          | 0.56      \n",
      "1          | 1          | 0.02       | 0.76          | 0.22      \n",
      "1          | 1          | 0.01       | 0.85          | 0.13      \n",
      "1          | 1          | 0.0        | 0.69          | 0.3       \n",
      "1          | 1          | 0.01       | 0.75          | 0.24      \n",
      "0          | 0          | 0.99       | 0.01          | 0.0       \n",
      "1          | 1          | 0.02       | 0.72          | 0.26      \n",
      "1          | 1          | 0.03       | 0.86          | 0.11      \n",
      "0          | 0          | 0.94       | 0.06          | 0.0       \n",
      "0          | 0          | 0.99       | 0.01          | 0.0       \n",
      "2          | 2          | 0.0        | 0.17          | 0.83      \n",
      "1          | 1          | 0.04       | 0.71          | 0.25      \n",
      "0          | 0          | 0.98       | 0.02          | 0.0       \n",
      "0          | 0          | 0.96       | 0.04          | 0.0       \n",
      "2          | 2          | 0.0        | 0.35          | 0.65      \n",
      "0          | 0          | 1.0        | 0.0           | 0.0       \n",
      "0          | 0          | 0.99       | 0.01          | 0.0       \n",
      "1          | 1          | 0.02       | 0.87          | 0.11      \n",
      "1          | 1          | 0.09       | 0.9           | 0.02      \n",
      "0          | 0          | 0.97       | 0.03          | 0.0       \n",
      "2          | 2          | 0.0        | 0.21          | 0.79      \n",
      "1          | 1          | 0.06       | 0.69          | 0.25      \n",
      "0          | 0          | 0.98       | 0.02          | 0.0       \n",
      "2          | 2          | 0.0        | 0.35          | 0.65      \n",
      "2          | 2          | 0.0        | 0.04          | 0.96      \n",
      "1          | 1          | 0.07       | 0.81          | 0.11      \n",
      "0          | 0          | 0.97       | 0.03          | 0.0       \n",
      "1          | 2          | 0.0        | 0.42          | 0.58      \n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Print results \n",
    "probs_y = np.round(probs_y, 2)\n",
    "res = \"{:<10} | {:<10} | {:<10} | {:<13} | {:<5}\".format(\"y_test\", \"y_pred\", \"Setosa(%)\", \"versicolor(%)\", \"virginica(%)\\n\")\n",
    "res += \"-\"*65+\"\\n\"\n",
    "res += \"\\n\".join(\"{:<10} | {:<10} | {:<10} | {:<13} | {:<10}\".format(x, y, a, b, c) for x, y, a, b, c in zip(y_test, y_pred, probs_y[:,0], probs_y[:,1], probs_y[:,2]))\n",
    "res += \"\\n\"+\"-\"*65+\"\\n\"\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "#metrics\n",
    "# print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "# print('Precision Score : ' + str(precision_score(y_test,y_pred, average='macro')))\n",
    "# print('Recall Score : ' + str(recall_score(y_test,y_pred, average='macro')))\n",
    "# print('F1 Score : ' + str(f1_score(y_test,y_pred, average='macro')))\n",
    "\n",
    "labels = np.unique(y_pred)\n",
    "print(classification_report(y_test, y_pred, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.9736842105263158\n",
      "Precision Score : 0.9736842105263158\n",
      "Recall Score : 0.9736842105263158\n",
      "F1 Score : 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from decimal import *\n",
    "def accuracy(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Classification accuracy is a ratio of the number of correct predictions out of all predictions that were made.\n",
    "    accuracy = TP+TN / FP+FN+TP+TN\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(yTest)) \n",
    "\n",
    "# Precision\n",
    "def precision(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Precision is the ratio between the true positives and all the points that are classified as positives.\n",
    "    precision = TP/(TP + FP)\n",
    "    \"\"\"\n",
    "\n",
    "    tp2 = 0\n",
    "    fp2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fp2 += 1\n",
    "    \n",
    "    return float(tp2 / (tp2 + fp2))\n",
    "\n",
    "# Recall\n",
    "def recall(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Recall is the measure of the model correctly identifying true positives. \n",
    "    recall = TP/(TP + FN)\n",
    "    \"\"\"\n",
    "\n",
    "    tp2 = 0\n",
    "    fn2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fn2 += 1\n",
    "    \n",
    "    return float(tp2 / (tp2 + fn2))\n",
    "\n",
    "# F1 score\n",
    "def F1score(yTest, yPred):\n",
    "    \"\"\"\n",
    "    F1 score is the combination of precision and recall. \n",
    "    F1 score = (2 * precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "\n",
    "    Precision = precision(yTest, yPred)\n",
    "    Recall = recall(yTest, yPred)\n",
    "\n",
    "    return (2 * Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "print('Accuracy Score : ' + str(accuracy(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(F1score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR [1.     0.9375 1.    ]\n",
      "TNR [1.         1.         0.96551724]\n",
      "PPV [1.  1.  0.9]\n",
      "NPV [1.         0.95652174 1.        ]\n",
      "FNR [0.     0.0625 0.    ]\n",
      "ACC [1.         0.97368421 0.97368421]\n",
      " \n",
      " \n",
      "accuracy [1.         0.97368421 0.97368421]\n",
      "precision [1.  1.  0.9]\n",
      "recall [1.     0.9375 1.    ]\n",
      "f1score [1.         0.96774194 0.94736842]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "#accuracy\n",
    "accuracy = (TP + TN)/(FP + FN + TP + TN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"TPR\", TPR)\n",
    "print(\"TNR\", TNR)\n",
    "print(\"PPV\", PPV)\n",
    "print(\"NPV\", NPV)\n",
    "print(\"FNR\", FNR)\n",
    "print(\"ACC\", ACC)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"accuracy\", accuracy)\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)\n",
    "print(\"f1score\", f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(xTrain)\n",
    "# X_test = sc.transform(xTest)\n",
    "\n",
    "\n",
    "# classifier = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "# classifier.fit(X_train, yTrain)\n",
    "\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # labels = np.unique(y_pred)\n",
    "# # print(accuracy(yTest, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
