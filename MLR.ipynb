{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(dataset, y, split=0.75):\n",
    "    \"\"\" \n",
    "    Separating the dataset into 2 parts: Training Dataset (to train the model) & Test Dataset (to evaluate the performance of the model)\n",
    "    The rows assigned to each dataset are randomly selected (to ensure that the model is objective).\n",
    "    randrange() generate a random integer in the range between 0 and the size of the list.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to split as a list of lists\n",
    "        split: Split percentage. (default split = 60%) --> A 60/40 for train/test\n",
    "        \n",
    "    Returns:\n",
    "        train: 60% of the dataset\n",
    "        test: The rows that remain in the copy of the dataset are then returned as the test dataset. (40%)\n",
    "    \"\"\"\n",
    "\n",
    "    #calculate how many rows the training set requires\n",
    "    xTrain = pd.DataFrame()\n",
    "    yTrain = pd.DataFrame()\n",
    "    trainSize = split * len(dataset)\n",
    "    datasetCopy = dataset.copy()\n",
    "    yCopy = y.copy()\n",
    "\n",
    "    #add index column\n",
    "    datasetCopy.reset_index(inplace=True)\n",
    "    datasetCopy = datasetCopy.rename(columns={\"index\": \"index\"})\n",
    "    yCopy.reset_index(inplace=True)\n",
    "    yCopy = yCopy.rename(columns={\"index\": \"index\"})\n",
    "\n",
    "    idxRan = len(datasetCopy)\n",
    "    while len(xTrain) < trainSize: # while until the train dataset contains the target number of rows.\n",
    "        randomIndex = np.random.choice(datasetCopy.index, 1, replace=False) #select random rows\n",
    "\n",
    "        datasetCopy = datasetCopy.drop(datasetCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "        yCopy = yCopy.drop(yCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "\n",
    "        xTrain = pd.concat([xTrain, dataset.loc[randomIndex]]) #add rows to train dataset\n",
    "        yTrain = pd.concat([yTrain, y.loc[randomIndex]]) #add rows to train dataset\n",
    "        idxRan = idxRan - 1\n",
    "    \n",
    "    datasetCopy = datasetCopy.drop(labels=[\"index\"], axis=1)\n",
    "    yCopy = yCopy.drop(labels=[\"index\"], axis=1)\n",
    "    return xTrain, datasetCopy, yTrain, yCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file (str, optional): Location of Iris.csv file. Defaults to 'Iris.csv'.\n",
    "        binary_version (bool, optional): Select if binary labels are used. Defaults to True.\n",
    "            target variable will select the positive label.\n",
    "\n",
    "    Returns:\n",
    "        array: Features and labels in the las column.\n",
    "        dict: Encodig of labels to string.\n",
    "    \"\"\"\n",
    "    iris = datasets.load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "    y = pd.DataFrame(iris.target, columns = [\"Target\"])\n",
    "\n",
    "    xTrain, xTest, yTrain, yTest = splitTrainTest(df,y)\n",
    "\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandarScaler:\n",
    "    \"\"\" \n",
    "    Standardize features by removing the mean and scaling to unit variance. \n",
    "    z = (x - MEAN) / DESV EST \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean(axis = 0).to_numpy()\n",
    "        self.std = X.std(axis = 0).to_numpy()\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X -= self.mean\n",
    "        X /= self.std\n",
    "        return X \n",
    "    \n",
    "    def fitTransform(self, X):\n",
    "        self.fit(X)\n",
    "        df = self.transform(X)\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    \"\"\"\n",
    "    Converts the training data into a series of ones and zeros for the classes given.\n",
    "    \"\"\"\n",
    "    yEncoded = np.zeros(shape=(y.size, int(y.max()[0])+1))\n",
    "    y = y.Target.tolist()\n",
    "    for i in range(len(y)): # rows\n",
    "        yEncoded[i,y[i]] = 1 # sub fila, sub columna que diga y\n",
    "    return yEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticReg():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softMaxFunc(self, z):\n",
    "        # sigm = []\n",
    "        # for i in range(len(z)):\n",
    "        #     lista = np.exp(z[i])/sum(np.exp(z[i]))\n",
    "        #     sigm.append(lista.tolist())\n",
    "        sigm = np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "        return sigm\n",
    "    \n",
    "    def getWeights(self,X,y):\n",
    "        cantTargs = np.shape(y)[1]\n",
    "        feats = np.shape(X)[1] # columnas \n",
    "        weight = np.random.rand(feats, cantTargs) # array FEATURE * CLASES\n",
    "        b = np.ones(cantTargs)\n",
    "        return weight, b\n",
    "\n",
    "    def fit(self,X,y, rounds = 1000, lRate=0.1):\n",
    "        \"\"\"\n",
    "        weighted sum of the inputs plus a bias term \n",
    "        For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ \n",
    "        handle multinomial loss; SOFTMAX\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight, self.bias = self.gradientDescent(rounds, lRate, X, y)\n",
    "            \n",
    "    def gradientDescent(self, rounds, lRate, X, y):\n",
    "        weight, bias = self.getWeights(X, y)\n",
    "        losses = []\n",
    "        rows = X.shape[0]\n",
    "        count = 0\n",
    "\n",
    "        while count < rounds:\n",
    "            z = np.dot(X, weight) + bias # (w·x)\n",
    "            yG = self.softMaxFunc(z) # yˆ = σ(w·x) Ya son probs\n",
    "            gradient = 1/rows * np.dot(X.T, (yG - y))\n",
    "            #updtate a lo weights\n",
    "            weight -= lRate * gradient\n",
    "            bias -= lRate * np.sum((yG - y))\n",
    "            count += 1 \n",
    "\n",
    "            loss = self.lossFunction(z, y) # loss entre ygorrito y y train\n",
    "            losses.append(loss)\n",
    "\n",
    "        self.lossSteps = losses\n",
    "        return weight, bias\n",
    "\n",
    "    def lossFunction(self, z, y):\n",
    "        \"\"\"\n",
    "        Calculate log negative likelihood (loss)\n",
    "        The loss increases as the predicted probability diverge from the actual label.\n",
    "\n",
    "        Parameters:\n",
    "        yG:\n",
    "        y:\n",
    "\n",
    "        Returns:\n",
    "        loss: Average loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Y must be one-hot encoded\n",
    "        rows = y.shape[0]\n",
    "        loglLoss = 1/rows * (np.trace(np.dot(z, y.T)) + np.sum(np.log(np.sum(np.exp(z), axis=1))))\n",
    "        return loglLoss\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weight) + self.bias\n",
    "        probs = self.softMaxFunc(z)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def predictProba(self, X):\n",
    "        z = np.dot(X, self.weight) + self.bias\n",
    "        probs = self.softMaxFunc(z)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandarScaler()\n",
    "\n",
    "xTrain = ss.fitTransform(xTrain)\n",
    "xTest = ss.fitTransform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = oneHot(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = MultiLogisticReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = mlr.predict(xTest)\n",
    "yProbs = mlr.predictProba(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = yPred.tolist()\n",
    "yTest = yTest.Target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(yTest, yPred):\n",
    "    cnfMatrix = confusion_matrix(yTest, yPred)\n",
    "\n",
    "    FP = (cnfMatrix.sum(axis=0) - np.diag(cnfMatrix)).astype(float) \n",
    "    FN = (cnfMatrix.sum(axis=1) - np.diag(cnfMatrix)).astype(float)\n",
    "    TP = (np.diag(cnfMatrix)).astype(float)\n",
    "    TN = (cnfMatrix.sum() - (FP + FN + TP)).astype(float)\n",
    "\n",
    "    #accuracy\n",
    "    accuracy = (TP + TN)/(FP + FN + TP + TN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return (print(\"Accuracy\", accuracy, \"\\n\\nPrecision\", precision, \"\\n\\nRecall\", recall, \"\\n\\nF1 score\", f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy [1.         0.89189189 0.89189189] \n",
      "\n",
      "Precision [1.   1.   0.75] \n",
      "\n",
      "Recall [1.         0.66666667 1.        ] \n",
      "\n",
      "F1 score [1.         0.8        0.85714286]\n"
     ]
    }
   ],
   "source": [
    "metrics(yTest, yPred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34f0680f93e83a4a101ea3928db54ef43df704f0ed6210e5ae2b09f0ef086431"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
