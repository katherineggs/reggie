{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(dataset, y, split=0.60):\n",
    "    \"\"\" \n",
    "    Separating the dataset into 2 parts: Training Dataset (to train the model) & Test Dataset (to evaluate the performance of the model)\n",
    "    The rows assigned to each dataset are randomly selected (to ensure that the model is objective).\n",
    "    randrange() generate a random integer in the range between 0 and the size of the list.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to split as a list of lists\n",
    "        split: Split percentage. (default split = 60%) --> A 60/40 for train/test\n",
    "        \n",
    "    Returns:\n",
    "        train: 60% of the dataset\n",
    "        test: The rows that remain in the copy of the dataset are then returned as the test dataset. (40%)\n",
    "    \"\"\"\n",
    "\n",
    "    #calculate how many rows the training set requires\n",
    "    xTrain = pd.DataFrame()\n",
    "    yTrain = pd.DataFrame()\n",
    "    trainSize = split * len(dataset)\n",
    "    datasetCopy = dataset.copy()\n",
    "    yCopy = y.copy()\n",
    "\n",
    "    #add index column\n",
    "    datasetCopy.reset_index(inplace=True)\n",
    "    datasetCopy = datasetCopy.rename(columns={\"index\": \"index\"})\n",
    "    yCopy.reset_index(inplace=True)\n",
    "    yCopy = yCopy.rename(columns={\"index\": \"index\"})\n",
    "\n",
    "    idxRan = len(datasetCopy)\n",
    "    while len(xTrain) < trainSize: # while until the train dataset contains the target number of rows.\n",
    "        randomIndex = np.random.choice(datasetCopy.index, 1, replace=False) #select random rows\n",
    "\n",
    "        datasetCopy = datasetCopy.drop(datasetCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "        yCopy = yCopy.drop(yCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "\n",
    "        xTrain = pd.concat([xTrain, dataset.loc[randomIndex]]) #add rows to train dataset\n",
    "        yTrain = pd.concat([yTrain, y.loc[randomIndex]]) #add rows to train dataset\n",
    "        idxRan = idxRan - 1\n",
    "    \n",
    "    datasetCopy = datasetCopy.drop(labels=[\"index\"], axis=1)\n",
    "    yCopy = yCopy.drop(labels=[\"index\"], axis=1)\n",
    "    return xTrain, datasetCopy, yTrain, yCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file (str, optional): Location of Iris.csv file. Defaults to 'Iris.csv'.\n",
    "        binary_version (bool, optional): Select if binary labels are used. Defaults to True.\n",
    "            target variable will select the positive label.\n",
    "\n",
    "    Returns:\n",
    "        array: Features and labels in the las column.\n",
    "        dict: Encodig of labels to string.\n",
    "    \"\"\"\n",
    "    iris = datasets.load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "    y = pd.DataFrame(iris.target, columns = [\"Target\"])\n",
    "\n",
    "    xTrain, xTest, yTrain, yTest = splitTrainTest(df,y)\n",
    "\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandarScaler:\n",
    "    \"\"\" \n",
    "    Standardize features by removing the mean and scaling to unit variance. \n",
    "    z = (x - MEAN) / DESV EST \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean(axis = 0).to_numpy()\n",
    "        self.std = X.std(axis = 0).to_numpy()\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X -= self.mean\n",
    "        X /= self.std\n",
    "        return X \n",
    "    \n",
    "    def fitTransform(self, X):\n",
    "        self.fit(X)\n",
    "        df = self.transform(X)\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    \"\"\"\n",
    "    Converts the training data into a series of ones and zeros for the classes given.\n",
    "    \"\"\"\n",
    "    yEncoded = np.zeros(shape=(y.size, int(y.max()[0])+1))\n",
    "    y = y.Target.tolist()\n",
    "    for i in range(len(y)): # rows\n",
    "        yEncoded[i,y[i]] = 1 # sub fila, sub columna que diga y\n",
    "    return yEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticReg():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softMaxFunc(self, z):\n",
    "        sigm = []\n",
    "        for i in range(len(z)):\n",
    "            lista = np.exp(z[i])/sum(np.exp(z[i]))\n",
    "            sigm.append(lista.tolist())\n",
    "        return sigm\n",
    "    \n",
    "    def getWeights(self,X,y):\n",
    "        cantTargs = np.shape(y)[1]\n",
    "        feats = np.shape(X)[1] # columnas \n",
    "        self.weight = np.random.rand(feats, cantTargs) # array FEATURE * CLASES\n",
    "\n",
    "    def fit(self,X,y, rounds = 1000, lRate=0.01):\n",
    "        \"\"\"\n",
    "        weighted sum of the inputs plus a bias term \n",
    "        For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ \n",
    "        handle multinomial loss; SOFTMAX\n",
    "        \"\"\"\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        \n",
    "        self.getWeights(X,y)\n",
    "        weight = self.weight # a optimizar # random initialization of w a\n",
    "\n",
    "        weight = self.gradientDescent(rounds, weight, lRate, y)\n",
    "            \n",
    "    def gradientDescent(self, rounds, weight, lRate, y):\n",
    "        losses = []\n",
    "        rows = y.shape[0]\n",
    "        count = 0\n",
    "        while count < rounds:\n",
    "            z = np.dot(self.X, weight) # (w·x)\n",
    "            yG = self.softMaxFunc(z) # yˆ = σ(w·x) Ya son probs\n",
    "\n",
    "            loss = self.lossFunction(z, y) # loss entre ygorrito y y train\n",
    "            losses.append(loss)\n",
    "\n",
    "            gradient = 1/ rows * np.dot(self.X.T, (y - yG))\n",
    "\n",
    "            #updtate a lo weights\n",
    "            weight -= lRate * gradient\n",
    "            count += 1 \n",
    "\n",
    "        self.lossSteps = losses\n",
    "        self.weight = weight\n",
    "        return weight\n",
    "\n",
    "    def lossFunction(self, z, y):\n",
    "        \"\"\"\n",
    "        Calculate cross-entropy loss\n",
    "        The loss increases as the predicted probability diverge from the actual label.\n",
    "\n",
    "        Parameters:\n",
    "        yG:\n",
    "        y:\n",
    "\n",
    "        Returns:\n",
    "        loss: Average cross entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Y must be one-hot encoded\n",
    "        rows = y.shape[0]\n",
    "        loglLoss = 1/rows * (np.trace(np.dot(z, y.T)) + np.sum(np.log(np.sum(np.exp(z), axis=1))))\n",
    "        return loglLoss\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weight)\n",
    "        P = self.softMaxFunc(z)\n",
    "        predictions = np.argmax(P, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def predictProba(self, X):\n",
    "        z = np.dot(X, self.weight)\n",
    "        probs = self.softMaxFunc(z)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandarScaler()\n",
    "\n",
    "xTrain = ss.fitTransform(xTrain)\n",
    "xTest = ss.fitTransform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = oneHot(yTrain)\n",
    "yTest = oneHot(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = MultiLogisticReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = mlr.predict(xTest)\n",
    "yProbs = mlr.predictProba(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Classification accuracy is a ratio of the number of correct predictions out of all predictions that were made.\n",
    "    accuracy = TP+TN / FP+FN+TP+TN\n",
    "    \"\"\"\n",
    "    # -- OPCION 1 --\n",
    "    #TRUE positive\n",
    "    TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    #FALSE positive\n",
    "    FP = sum((yTest == 0) & (yPred == 1))\n",
    "\n",
    "    #FALSE positive\n",
    "    FN = sum((yTest == 1) & (yPred == 0))\n",
    "\n",
    "    #TRUE negative\n",
    "    TN = sum((yTest == 0) & (yPred == 0))\n",
    "    \n",
    "    return (TP + TN)/(FP + FN + TP + TN)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    correct = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            correct += 1\n",
    "    # return correct / float(len(yTest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Precision is the ratio between the true positives and all the points that are classified as positives.\n",
    "    precision = TP/(TP + FP)\n",
    "    \"\"\"\n",
    "\n",
    "    #TRUE positive\n",
    "    TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    #FALSE positive\n",
    "    FP = sum((yTest == 0) & (yPred == 1))\n",
    "\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    tp2 = 0\n",
    "    fp2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fp2 += 1 \n",
    "    # return float(tp2 / (tp2 + fp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Recall is the measure of the model correctly identifying true positives. \n",
    "    recall = TP/(TP + FN)\n",
    "    \"\"\"\n",
    "\n",
    "    #TRUE positive\n",
    "    TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    #FALSE negative\n",
    "    FN = sum((yTest == 1) & (yPred == 0))\n",
    "\n",
    "    return TP / float(TP + FN)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    tp2 = 0\n",
    "    fn2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fn2 += 1\n",
    "    # return float(tp2 / (tp2 + fn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1score(yTest, yPred):\n",
    "    \"\"\"\n",
    "    F1 score is the combination of precision and recall. \n",
    "    F1 score = (2 * precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "\n",
    "    Precision = precision(yTest, yPred)\n",
    "    Recall = recall(yTest, yPred)\n",
    "\n",
    "    return (2 * Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherinegarcia/opt/anaconda3/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(xTrain)\n",
    "X_test = sc.transform(xTest)\n",
    "\n",
    "oh = OneHotEncoder()\n",
    "# y_Train =  yTrain #oh.fit_transform(yTrain).toarray()\n",
    "# y_Test =  oh.fit_transform(yTest).toarray()\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "classifier.fit(X_train, yTrain)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_probs = classifier.predict_proba(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "       0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yPred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34f0680f93e83a4a101ea3928db54ef43df704f0ed6210e5ae2b09f0ef086431"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
