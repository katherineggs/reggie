{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(dataset, y, split=0.75):\n",
    "    \"\"\" \n",
    "    Separating the dataset into 2 parts: Training Dataset (to train the model) & Test Dataset (to evaluate the performance of the model)\n",
    "    The rows assigned to each dataset are randomly selected (to ensure that the model is objective).\n",
    "    randrange() generate a random integer in the range between 0 and the size of the list.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to split as a list of lists\n",
    "        split: Split percentage. (default split = 60%) --> A 60/40 for train/test\n",
    "        \n",
    "    Returns:\n",
    "        train: 60% of the dataset\n",
    "        test: The rows that remain in the copy of the dataset are then returned as the test dataset. (40%)\n",
    "    \"\"\"\n",
    "\n",
    "    #calculate how many rows the training set requires\n",
    "    xTrain = pd.DataFrame()\n",
    "    yTrain = pd.DataFrame()\n",
    "    trainSize = split * len(dataset)\n",
    "    datasetCopy = dataset.copy()\n",
    "    yCopy = y.copy()\n",
    "\n",
    "    #add index column\n",
    "    datasetCopy.reset_index(inplace=True)\n",
    "    datasetCopy = datasetCopy.rename(columns={\"index\": \"index\"})\n",
    "    yCopy.reset_index(inplace=True)\n",
    "    yCopy = yCopy.rename(columns={\"index\": \"index\"})\n",
    "\n",
    "    idxRan = len(datasetCopy)\n",
    "    while len(xTrain) < trainSize: # while until the train dataset contains the target number of rows.\n",
    "        randomIndex = np.random.choice(datasetCopy.index, 1, replace=False) #select random rows\n",
    "\n",
    "        datasetCopy = datasetCopy.drop(datasetCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "        yCopy = yCopy.drop(yCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "\n",
    "        xTrain = pd.concat([xTrain, dataset.loc[randomIndex]]) #add rows to train dataset\n",
    "        yTrain = pd.concat([yTrain, y.loc[randomIndex]]) #add rows to train dataset\n",
    "        idxRan = idxRan - 1\n",
    "    \n",
    "    datasetCopy = datasetCopy.drop(labels=[\"index\"], axis=1)\n",
    "    yCopy = yCopy.drop(labels=[\"index\"], axis=1)\n",
    "    return xTrain, datasetCopy, yTrain, yCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file (str, optional): Location of Iris.csv file. Defaults to 'Iris.csv'.\n",
    "        binary_version (bool, optional): Select if binary labels are used. Defaults to True.\n",
    "            target variable will select the positive label.\n",
    "\n",
    "    Returns:\n",
    "        array: Features and labels in the las column.\n",
    "        dict: Encodig of labels to string.\n",
    "    \"\"\"\n",
    "    iris = datasets.load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "    y = pd.DataFrame(iris.target, columns = [\"Target\"])\n",
    "\n",
    "    xTrain, xTest, yTrain, yTest = splitTrainTest(df,y)\n",
    "\n",
    "    return xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandarScaler:\n",
    "    \"\"\" \n",
    "    Standardize features by removing the mean and scaling to unit variance. \n",
    "    z = (x - MEAN) / DESV EST \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean(axis = 0).to_numpy()\n",
    "        self.std = X.std(axis = 0).to_numpy()\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X -= self.mean\n",
    "        X /= self.std\n",
    "        return X \n",
    "    \n",
    "    def fitTransform(self, X):\n",
    "        self.fit(X)\n",
    "        df = self.transform(X)\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    \"\"\"\n",
    "    Converts the training data into a series of ones and zeros for the classes given.\n",
    "    \"\"\"\n",
    "    yEncoded = np.zeros(shape=(y.size, int(y.max()[0])+1))\n",
    "    y = y.Target.tolist()\n",
    "    for i in range(len(y)): # rows\n",
    "        yEncoded[i,y[i]] = 1 # sub fila, sub columna que diga y\n",
    "    return yEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLogisticReg():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def softMaxFunc(self, z):\n",
    "        # sigm = []\n",
    "        # for i in range(len(z)):\n",
    "        #     lista = np.exp(z[i])/sum(np.exp(z[i]))\n",
    "        #     sigm.append(lista.tolist())\n",
    "        sigm = np.exp(z) / np.sum(np.exp(z), axis = 1, keepdims = True)\n",
    "        return sigm\n",
    "    \n",
    "    def getWeights(self,X,y):\n",
    "        cantTargs = np.shape(y)[1]\n",
    "        feats = np.shape(X)[1] # columnas \n",
    "        weight = np.random.rand(feats, cantTargs) # array FEATURE * CLASES\n",
    "        b = np.ones(cantTargs)\n",
    "        return weight, b\n",
    "\n",
    "    def fit(self,X,y, rounds = 1000, lRate=0.1):\n",
    "        \"\"\"\n",
    "        weighted sum of the inputs plus a bias term \n",
    "        For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ \n",
    "        handle multinomial loss; SOFTMAX\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight, self.bias = self.gradientDescent(rounds, lRate, X, y)\n",
    "            \n",
    "    def gradientDescent(self, rounds, lRate, X, y):\n",
    "        weight, bias = self.getWeights(X, y)\n",
    "        losses = []\n",
    "        rows = X.shape[0]\n",
    "        count = 0\n",
    "\n",
    "        while count < rounds:\n",
    "            z = np.dot(X, weight) + bias # (w·x)\n",
    "            yG = self.softMaxFunc(z) # yˆ = σ(w·x) Ya son probs\n",
    "            gradient = 1/rows * np.dot(X.T, (yG - y))\n",
    "            #updtate a lo weights\n",
    "            weight -= lRate * gradient\n",
    "            bias -= lRate * np.sum((yG - y))\n",
    "            count += 1 \n",
    "\n",
    "            loss = self.lossFunction(z, y) # loss entre ygorrito y y train\n",
    "            losses.append(loss)\n",
    "\n",
    "        self.lossSteps = losses\n",
    "        return weight, bias\n",
    "\n",
    "    def lossFunction(self, z, y):\n",
    "        \"\"\"\n",
    "        Calculate cross-entropy loss\n",
    "        The loss increases as the predicted probability diverge from the actual label.\n",
    "\n",
    "        Parameters:\n",
    "        yG:\n",
    "        y:\n",
    "\n",
    "        Returns:\n",
    "        loss: Average cross entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Y must be one-hot encoded\n",
    "        rows = y.shape[0]\n",
    "        loglLoss = 1/rows * (np.trace(np.dot(z, y.T)) + np.sum(np.log(np.sum(np.exp(z), axis=1))))\n",
    "        return loglLoss\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weight) + self.bias\n",
    "        probs = self.softMaxFunc(z)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def predictProba(self, X):\n",
    "        z = np.dot(X, self.weight) + self.bias\n",
    "        probs = self.softMaxFunc(z)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandarScaler()\n",
    "\n",
    "xTrain = ss.fitTransform(xTrain)\n",
    "xTest = ss.fitTransform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = oneHot(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr = MultiLogisticReg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = mlr.predict(xTest)\n",
    "yProbs = mlr.predictProba(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = yPred.tolist()\n",
    "yTest = yTest.Target.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Classification accuracy is a ratio of the number of correct predictions out of all predictions that were made.\n",
    "    accuracy = TP+TN / FP+FN+TP+TN\n",
    "    \"\"\"\n",
    "    # # -- OPCION 1 --\n",
    "    # #TRUE positive\n",
    "    # TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    # #FALSE positive\n",
    "    # FP = sum((yTest == 0) & (yPred == 1))\n",
    "\n",
    "    # #FALSE positive\n",
    "    # FN = sum((yTest == 1) & (yPred == 0))\n",
    "\n",
    "    # #TRUE negative\n",
    "    # TN = sum((yTest == 0) & (yPred == 0))\n",
    "    \n",
    "    # return (TP + TN)/(FP + FN + TP + TN)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    correct = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(yTest)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108108108108109"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Precision is the ratio between the true positives and all the points that are classified as positives.\n",
    "    precision = TP/(TP + FP)\n",
    "    \"\"\"\n",
    "\n",
    "    #TRUE positive\n",
    "    TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    #FALSE positive\n",
    "    FP = sum((yTest == 0) & (yPred == 1))\n",
    "\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    tp2 = 0\n",
    "    fp2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fp2 += 1 \n",
    "    # return float(tp2 / (tp2 + fp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(yTest, yPred):\n",
    "    \"\"\"\n",
    "    Recall is the measure of the model correctly identifying true positives. \n",
    "    recall = TP/(TP + FN)\n",
    "    \"\"\"\n",
    "\n",
    "    #TRUE positive\n",
    "    TP = sum((yTest == 1) & (yPred == 1))\n",
    "\n",
    "    #FALSE negative\n",
    "    FN = sum((yTest == 1) & (yPred == 0))\n",
    "\n",
    "    return TP / float(TP + FN)\n",
    "\n",
    "    # -- OPCION 2 --\n",
    "    tp2 = 0\n",
    "    fn2 = 0\n",
    "    for i in range(len(yTest)):\n",
    "        if yTest[i] == yPred[i]:\n",
    "            tp2 += 1\n",
    "            \n",
    "        if yTest[i] != yPred[i]:\n",
    "            fn2 += 1\n",
    "    # return float(tp2 / (tp2 + fn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1score(yTest, yPred):\n",
    "    \"\"\"\n",
    "    F1 score is the combination of precision and recall. \n",
    "    F1 score = (2 * precision * recall) / (precision + recall)\n",
    "    \"\"\"\n",
    "\n",
    "    Precision = precision(yTest, yPred)\n",
    "    Recall = recall(yTest, yPred)\n",
    "\n",
    "    return (2 * Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy [0.97297297 0.86486486 0.89189189]\n",
      "precision [0.90909091 1.         0.76470588]\n",
      "recall [1.         0.64285714 1.        ]\n",
      "f1score [0.95238095 0.7826087  0.86666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(yTest, yPred)\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "#accuracy\n",
    "accuracy = (TP + TN)/(FP + FN + TP + TN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"accuracy\", accuracy)\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)\n",
    "print(\"f1score\", f1score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34f0680f93e83a4a101ea3928db54ef43df704f0ed6210e5ae2b09f0ef086431"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
