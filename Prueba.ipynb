{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoder\n",
    "def oneHot(y):\n",
    "    \"\"\"\n",
    "    Converts the training data into a series of ones and zeros for the classes given.\n",
    "    \"\"\"\n",
    "    yEncoded = np.zeros((y.size, y.max()+1))\n",
    "    yEncoded[np.arange(y.size),y] = 1\n",
    "    return yEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandarScaler:\n",
    "    \"\"\" \n",
    "    Standardize features by removing the mean and scaling to unit variance. \n",
    "    z = (x - MEAN) / DESV EST \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.mean = X.mean(axis = 0).to_numpy()\n",
    "        self.std = X.std(axis = 0).to_numpy()\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X -= self.mean\n",
    "        X /= self.std\n",
    "        return X \n",
    "    \n",
    "    def fitTransform(self, X):\n",
    "        self.fit(X)\n",
    "        df = self.transform(X)\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into a train and test set\n",
    "def splitTrainTest(dataset, split=0.60):\n",
    "    \"\"\" \n",
    "    Separating the dataset into 2 parts: Training Dataset (to train the model) & Test Dataset (to evaluate the performance of the model)\n",
    "    The rows assigned to each dataset are randomly selected (to ensure that the model is objective).\n",
    "    randrange() generate a random integer in the range between 0 and the size of the list.\n",
    "\n",
    "    Parameters:\n",
    "        dataset: The dataset to split as a list of lists\n",
    "        split: Split percentage. (default split = 60%) --> A 60/40 for train/test\n",
    "        \n",
    "    Returns:\n",
    "        train: 60% of the dataset\n",
    "        test: The rows that remain in the copy of the dataset are then returned as the test dataset. (40%)\n",
    "    \"\"\"\n",
    "\n",
    "    #calculate how many rows the training set requires\n",
    "    train = pd.DataFrame()\n",
    "    trainSize = split * len(dataset)\n",
    "    datasetCopy = dataset.copy()\n",
    "\n",
    "    #add index column\n",
    "    datasetCopy.reset_index(inplace=True)\n",
    "    datasetCopy = datasetCopy.rename(columns={\"index\": \"index\"})\n",
    "\n",
    "    idxRan = len(datasetCopy)\n",
    "    while len(train) < trainSize: # while until the train dataset contains the target number of rows.\n",
    "        randomIndex = np.random.choice(datasetCopy.index, 1, replace=False) #select random rows\n",
    "        datasetCopy = datasetCopy.drop(datasetCopy[\"index\"][randomIndex]) #remove random rows from the datasetCopy\n",
    "        train = pd.concat([train, dataset.loc[randomIndex]]) #add rows to train dataset\n",
    "        idxRan = idxRan - 1\n",
    "    \n",
    "    return train, datasetCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07438118377140324\n"
     ]
    }
   ],
   "source": [
    "#loss\n",
    "def lossFunction(probs, target):\n",
    "        \"\"\"\n",
    "        Calculates cross entropy loss for a set of predictions and actual targets.\n",
    "        Cross-entropy is a measure of the difference between two probability distributions for a given random variable\n",
    "\n",
    "        Parameters:\n",
    "        probs: Probability predictions in MultiLogisticReg\n",
    "        target: Actual target values\n",
    "\n",
    "        Returns:\n",
    "        loss: Average cross entropy loss\n",
    "        \"\"\"\n",
    "\n",
    "        if target == 1:\n",
    "            return -np.log(probs)\n",
    "        else:\n",
    "            return -np.log(1 - probs)\n",
    "\n",
    "\n",
    "def lossFunc(y, yG):\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy loss\n",
    "    The loss increases as the predicted probability diverge from the actual label.\n",
    "\n",
    "    Parameters:\n",
    "    yG:\n",
    "    y:\n",
    "\n",
    "    Returns:\n",
    "    loss: Average cross entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Y must be one-hot encoded\n",
    "    rows = yG.shape[0]\n",
    "    loss =- np.sum(y * np.log(yG))\n",
    "\n",
    "    return loss/float(rows)\n",
    "\n",
    "\n",
    "y = np.array([0,0,1]) \n",
    "yG = np.array([0.1,0.1,0.8])\n",
    "\n",
    "print(lossFunc(y, yG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit - gradient descent "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81f78be8e84c422d020e02bed99d970b4c01c3a614224aa9b2e56590be3a6b2a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
